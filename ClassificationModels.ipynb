{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bfef3db",
   "metadata": {},
   "source": [
    "## Classification models\n",
    "\n",
    "In this file we will train some classic classification models for comparisson with the BERT model we trained for the first screening of resumes.\n",
    "\n",
    "We will be training a decision tree, and random forest model and a logistic regression classifier.\n",
    "\n",
    "These are all lighter easier to load than the NLP BERT model we trained earlier, but can still yield decent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93411c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2d2ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pickled dataset\n",
    "import pickle\n",
    "with open('Data/Dataframes/newDF.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0dd2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we did in the previous notebook, we will drop the columns not used for training\n",
    "trainingDF = df.drop(columns=['ID', 'Label', 'TextLen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba6fe413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Resume', 'Software_Developer', 'Database_Administrator',\n",
       "       'Systems_Administrator', 'Project_manager', 'Web_Developer',\n",
       "       'Network_Administrator', 'Security_Analyst', 'Python_Developer',\n",
       "       'Java_Developer', 'Front_End_Developer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we have the correct columns\n",
    "trainingDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce1772d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainingDF['Resume']\n",
    "y = trainingDF.drop(columns=['Resume'])\n",
    "\n",
    "# Splitting the dataset into training and testing sets. 10 % of the data will be used for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21f2da9",
   "metadata": {},
   "source": [
    "### Vectorization of text data\n",
    "\n",
    "We are again working with resume texts, which we cant just feed into any ML model.\n",
    "\n",
    "First we have to vectorize it. We will use the TfidfVectorizer, which is common for these types of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38488e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cap the number of features of the vector to 5000 to start with, but we might return to this later\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, labels):\n",
    "    print(f\"\\n{name}: \")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Decision Tree ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pelle\\Work\\1semSoft\\exam\\AIML-Exam\\examVenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pelle\\Work\\1semSoft\\exam\\AIML-Exam\\examVenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pelle\\Work\\1semSoft\\exam\\AIML-Exam\\examVenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "    Software_Developer       0.94      0.94      0.94      1451\n",
      "Database_Administrator       0.84      0.82      0.83       299\n",
      " Systems_Administrator       0.73      0.76      0.74       569\n",
      "       Project_manager       0.72      0.76      0.74       437\n",
      "         Web_Developer       0.74      0.75      0.75       605\n",
      " Network_Administrator       0.75      0.70      0.72       444\n",
      "      Security_Analyst       0.79      0.80      0.79       314\n",
      "      Python_Developer       0.90      0.85      0.87       269\n",
      "        Java_Developer       0.87      0.85      0.86       329\n",
      "   Front_End_Developer       0.84      0.85      0.85       387\n",
      "\n",
      "             micro avg       0.83      0.83      0.83      5104\n",
      "             macro avg       0.81      0.81      0.81      5104\n",
      "          weighted avg       0.83      0.83      0.83      5104\n",
      "           samples avg       0.81      0.82      0.79      5104\n",
      "\n",
      "\n",
      "=== Logistic Regression ===\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "    Software_Developer       0.97      0.94      0.96      1451\n",
      "Database_Administrator       0.97      0.72      0.83       299\n",
      " Systems_Administrator       0.86      0.71      0.77       569\n",
      "       Project_manager       0.90      0.70      0.79       437\n",
      "         Web_Developer       0.81      0.72      0.76       605\n",
      " Network_Administrator       0.87      0.63      0.73       444\n",
      "      Security_Analyst       0.90      0.77      0.83       314\n",
      "      Python_Developer       0.95      0.78      0.86       269\n",
      "        Java_Developer       0.94      0.78      0.85       329\n",
      "   Front_End_Developer       0.95      0.80      0.87       387\n",
      "\n",
      "             micro avg       0.92      0.79      0.85      5104\n",
      "             macro avg       0.91      0.76      0.82      5104\n",
      "          weighted avg       0.92      0.79      0.85      5104\n",
      "           samples avg       0.86      0.79      0.81      5104\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pelle\\Work\\1semSoft\\exam\\AIML-Exam\\examVenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pelle\\Work\\1semSoft\\exam\\AIML-Exam\\examVenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pelle\\Work\\1semSoft\\exam\\AIML-Exam\\examVenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluating the decision tree classifier\n",
    "dt_model = MultiOutputClassifier(DecisionTreeClassifier(random_state=42))\n",
    "evaluate_model(\"Decision Tree\", dt_model, X_train_vec, y_train, X_test_vec, y_test, y.columns)\n",
    "\n",
    "# Training and evaluating the Logistic regression classifier\n",
    "lr_model = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "evaluate_model(\"Logistic Regression\", lr_model, X_train_vec, y_train, X_test_vec, y_test, y.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321bf8a1",
   "metadata": {},
   "source": [
    "### Grid searching for the best parameters for the Random Forest Classifier\n",
    "\n",
    "In our earlier projects we had good results with the RF classifier, but it sometimes requires tinkering with the parameters.\n",
    "\n",
    "We will therefore search for the best candidates with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c01f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "Best Parameters Found (Random Forest):\n",
      "{'estimator__max_depth': None, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 100}\n",
      "\n",
      "Classification Report for Optimized Random Forest:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "    Software_Developer       0.97      0.95      0.96      1451\n",
      "Database_Administrator       0.99      0.69      0.81       299\n",
      " Systems_Administrator       0.86      0.69      0.76       569\n",
      "       Project_manager       0.95      0.64      0.77       437\n",
      "         Web_Developer       0.82      0.72      0.76       605\n",
      " Network_Administrator       0.89      0.61      0.73       444\n",
      "      Security_Analyst       0.92      0.65      0.76       314\n",
      "      Python_Developer       0.99      0.74      0.85       269\n",
      "        Java_Developer       0.93      0.81      0.87       329\n",
      "   Front_End_Developer       0.95      0.81      0.88       387\n",
      "\n",
      "             micro avg       0.93      0.77      0.84      5104\n",
      "             macro avg       0.93      0.73      0.81      5104\n",
      "          weighted avg       0.93      0.77      0.84      5104\n",
      "           samples avg       0.85      0.77      0.79      5104\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pelle\\Work\\1semSoft\\exam\\AIML-Exam\\examVenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pelle\\Work\\1semSoft\\exam\\AIML-Exam\\examVenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\pelle\\Work\\1semSoft\\exam\\AIML-Exam\\examVenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Reusable function to run GridSearchCV for Random Forest\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [50, 100],\n",
    "    'estimator__max_depth': [None, 10, 30],\n",
    "    'estimator__min_samples_split': [2, 5],\n",
    "    'estimator__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "    # Setup the base model\n",
    "rf = MultiOutputClassifier(RandomForestClassifier(random_state=42))\n",
    "\n",
    "    # Setup Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    rf,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_micro',\n",
    "    verbose=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "    # Train the grid search\n",
    "grid_search.fit(X_train_vec, y_train)\n",
    "\n",
    "    # Print best parameters found\n",
    "print(\"\\nBest Parameters Found (Random Forest):\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "    # Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "print(\"\\nClassification Report for Optimized Random Forest:\")\n",
    "print(classification_report(y_test, y_pred, target_names=y.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d930a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Make sure the directory exists\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save vectorizer\n",
    "with open(\"models/classificationModels/vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Save models\n",
    "with open(\"models/classificationModels/decision_tree.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dt_model, f)\n",
    "\n",
    "with open(\"models/classificationModels/logistic_regression.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "\n",
    "with open(\"models/classificationModels/random_forest_best.pkl\", \"wb\") as f:\n",
    "    pickle.dump(grid_search.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797fa889",
   "metadata": {},
   "source": [
    "## Evaluation of the classic classification models\n",
    "To compare with the BERT model we trained earlier for first-round resume screening, we tested three classic machine learning models: Decision Tree, Logistic Regression, and Random Forest. All models were trained using TF-IDF vectors based on the resume texts and evaluated using standard multi-label classification metrics.\n",
    "\n",
    "#### Decision Tree\n",
    "The Decision Tree model was quick to train and easy to interpret, but the results were clearly weaker than the other models. It managed to score reasonably well on high-frequency labels like Software_Developer (F1-score: 0.94) and Python_Developer (0.87), but its performance dropped for several other categories such as Systems_Administrator (0.74) and Project_manager (0.74). The micro average F1-score ended at 0.83, which confirms that the model had trouble generalizing. Overall, the Decision Tree serves as a decent baseline but is not strong enough for a production-level resume screening task.\n",
    "\n",
    "#### Logistic Regression\n",
    "Logistic Regression performed much better across the board. It gave high precision and recall on the most common labels, and also handled the less frequent ones more consistently. For example, it achieved F1-scores of 0.96 for Software_Developer, 0.89 for Java_Developer, and 0.88 for Front_End_Developer. The micro average F1-score was 0.85, and the samples average F1-score landed at 0.81. These results suggest that Logistic Regression is a solid and efficient option for this kind of multi-label classification task.\n",
    "\n",
    "\n",
    "#### Random Forest (with GridSearch)\n",
    "To get the best results out of the Random Forest model, we performed a grid search over 24 combinations of parameters, resulting in 72 total fits. The best-performing parameters were:\n",
    "\n",
    "{\n",
    "    'estimator__max_depth': None,\n",
    "    'estimator__min_samples_leaf': 1,\n",
    "    'estimator__min_samples_split': 5,\n",
    "    'estimator__n_estimators': 100\n",
    "}\n",
    "\n",
    "With these parameters, the Random Forest model achieved the strongest overall results. It reached an F1-score of 0.96 for Software_Developer, 0.87 for Java_Developer, and 0.88 for Front_End_Developer. While performance dropped slightly on some of the less represented classes, such as Database_Administrator (0.81) and Security_Analyst (0.76), the overall micro average F1-score was 0.84, and the samples average F1-score was 0.79. This confirms that Random Forest, when tuned properly, can perform really well on this type of task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "examVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
